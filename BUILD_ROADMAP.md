# ðŸ—ºï¸ CareerCopilot AI - 12-Week Build Roadmap

## ðŸ“Š Overview

This roadmap breaks down the implementation into 6 phases over 12 weeks, prioritizing foundation â†’ core features â†’ AI integration â†’ advanced features.

**Total Time**: 12 weeks (3 months)  
**Team Size**: 2-3 full-stack engineers  
**Launch Strategy**: Gradual rollout with user feedback

---

## ðŸ—ï¸ Phase 1: Foundation (Weeks 1-2)

### Goal
Solid foundation with auth, database, and basic CRUD operations.

### Backend Tasks
- [x] âœ… Supabase project setup
- [x] âœ… Database schema (5 migrations)
- [x] âœ… RLS policies (all tables)
- [x] âœ… AI Orchestrator V2
- [x] âœ… Repository layer
- [ ] ðŸ”² Auth endpoints (Supabase integration)
- [ ] ðŸ”² Profile CRUD endpoints
- [ ] ðŸ”² Resume upload endpoint
- [ ] ðŸ”² File storage setup

### Frontend Tasks
- [ ] ðŸ”² Auth UI (Supabase Auth)
- [ ] ðŸ”² Onboarding flow (3 steps)
- [ ] ðŸ”² Basic dashboard layout
- [ ] ðŸ”² Resume upload component

### Deliverables
- âœ… Users can sign up/login
- âœ… Users can complete onboarding
- âœ… Users can upload resumes
- âœ… Files stored securely

### Success Criteria
- Auth success rate > 98%
- Upload success rate > 95%
- RLS blocks unauthorized access

---

## ðŸ”§ Phase 2: Resume Parsing & Editor (Weeks 3-4)

### Goal
Parse uploaded resumes and provide basic editing capabilities.

### Backend Tasks
- [ ] ðŸ”² PDF/DOCX text extraction
- [ ] ðŸ”² Deterministic section parser
- [ ] ðŸ”² Bullet point extractor
- [ ] ðŸ”² Signal computation engine
- [ ] ðŸ”² Resume CRUD endpoints
- [ ] ðŸ”² Version control logic
- [ ] ðŸ”² Parsing status endpoint

### Frontend Tasks
- [ ] ðŸ”² Parsing progress indicator
- [ ] ðŸ”² Parsing review screen
- [ ] ðŸ”² Resume editor UI
  - [ ] ðŸ”² Section management
  - [ ] ðŸ”² Bullet editing
  - [ ] ðŸ”² Real-time preview
- [ ] ðŸ”² Save/auto-save logic

### Deliverables
- âœ… Resumes parsed into structured format
- âœ… Users can edit all resume sections
- âœ… Changes saved with version control
- âœ… Parsing confidence shown

### Success Criteria
- Parsing accuracy > 90%
- Edit operations < 200ms
- Zero data loss on save

### Testing
- Test with 50+ diverse resumes
- Various formats (PDF, DOCX)
- Different layouts (1-column, 2-column)
- Edge cases (unusual fonts, tables)

---

## ðŸ¤– Phase 3: AI Integration (Weeks 5-6)

### Goal
Add AI-powered bullet improvement and explanations.

### Backend Tasks
- [ ] ðŸ”² Deploy AI skill: `improve_bullet`
- [ ] ðŸ”² Deploy AI skill: `explain_bullet_strength`
- [ ] ðŸ”² Endpoint: `POST /v1/resumes/bullets/{id}/improve`
- [ ] ðŸ”² Endpoint: `GET /v1/resumes/bullets/{id}/explain`
- [ ] ðŸ”² Endpoint: `POST /v1/ai/feedback`
- [ ] ðŸ”² Explanation storage logic
- [ ] ðŸ”² Cost tracking dashboard

### Frontend Tasks
- [ ] ðŸ”² "Improve with AI" button
- [ ] ðŸ”² AI suggestion modal
  - [ ] ðŸ”² Show original vs improved
  - [ ] ðŸ”² Display explanation
  - [ ] ðŸ”² Accept/reject buttons
- [ ] ðŸ”² Hover tooltip for explanations
- [ ] ðŸ”² Feedback UI (thumbs up/down)
- [ ] ðŸ”² Loading states

### Deliverables
- âœ… Users can get AI improvements for bullets
- âœ… Users see explanations on hover
- âœ… Users can provide feedback
- âœ… All AI activity logged

### Success Criteria
- AI response time < 3s
- Validation pass rate > 95%
- User acceptance rate > 60%
- Feedback collection rate > 30%

### Testing
- Internal testing with 10 users
- A/B test: AI suggestions on/off
- Measure: acceptance rate, edit rate, satisfaction

---

## ðŸ“ˆ Phase 4: Explainability & ATS (Weeks 7-8)

### Goal
Add resume heatmap and ATS readiness analysis.

### Backend Tasks
- [ ] ðŸ”² Deterministic heatmap engine
- [ ] ðŸ”² Deploy AI skill: `summarize_section_quality`
- [ ] ðŸ”² ATS simulation engine
  - [ ] ðŸ”² Format checks
  - [ ] ðŸ”² Parsing tests
  - [ ] ðŸ”² Keyword extraction
- [ ] ðŸ”² Deploy AI skill: `explain_ats_risk`
- [ ] ðŸ”² Endpoint: `GET /v1/resumes/{id}/heatmap`
- [ ] ðŸ”² Endpoint: `GET /v1/resumes/{id}/ats-analysis`

### Frontend Tasks
- [ ] ðŸ”² Resume heatmap visualization
  - [ ] ðŸ”² Color-coded sections
  - [ ] ðŸ”² Score display
  - [ ] ðŸ”² Summary tooltips
- [ ] ðŸ”² ATS readiness page
  - [ ] ðŸ”² Category breakdown
  - [ ] ðŸ”² Pass/warning/fail indicators
  - [ ] ðŸ”² Explanations per check
  - [ ] ðŸ”² Fix suggestions
- [ ] ðŸ”² Export checklist modal

### Deliverables
- âœ… Users see visual quality heatmap
- âœ… Users understand ATS risks
- âœ… Users get actionable fix suggestions
- âœ… Export validated before download

### Success Criteria
- Heatmap accuracy > 85%
- ATS predictions align with real ATS
- Users fix >70% of critical issues

### Testing
- Validate ATS checks with real ATS systems
- Test with 20+ job boards
- Compare with competitor tools

---

## ðŸŽ¯ Phase 5: Job Matching (Weeks 9-10)

### Goal
Match resumes to job descriptions and identify skill gaps.

### Backend Tasks
- [ ] ðŸ”² JD parser (extract skills, requirements)
- [ ] ðŸ”² Skill gap calculator (deterministic)
- [ ] ðŸ”² Deploy AI skill: `explain_skill_gaps`
- [ ] ðŸ”² ATS score RPC (deterministic)
- [ ] ðŸ”² Endpoint: `POST /v1/jobs/analyze`
- [ ] ðŸ”² Endpoint: `GET /v1/jobs/{id}/gaps`
- [ ] ðŸ”² Endpoint: `GET /v1/jobs/{id}/suggestions`
- [ ] ðŸ”² Deploy AI skill: `match_job` (existing)

### Frontend Tasks
- [ ] ðŸ”² Job description input page
- [ ] ðŸ”² Match analysis results page
  - [ ] ðŸ”² Match score visualization
  - [ ] ðŸ”² Matched skills list
  - [ ] ðŸ”² Missing skills list
  - [ ] ðŸ”² Gap explanations
  - [ ] ðŸ”² Prioritized actions
- [ ] ðŸ”² Resume tailoring suggestions
- [ ] ðŸ”² Multiple job tracking

### Deliverables
- âœ… Users can analyze job-resume fit
- âœ… Users see specific skill gaps
- âœ… Users get prioritized actions
- âœ… Users can track multiple jobs

### Success Criteria
- Match scores correlate with interviews
- Gap identification accuracy > 90%
- Users find suggestions helpful (>4/5)

### Testing
- Test with 100+ real job postings
- Validate against user interview outcomes
- Compare with LinkedIn matching

---

## ðŸš€ Phase 6: Advanced Features (Weeks 11-12)

### Goal
Add template recommendations and career copilot chat.

### Backend Tasks
- [ ] ðŸ”² Template system (50+ templates)
- [ ] ðŸ”² Deploy AI skill: `recommend_template`
- [ ] ðŸ”² Deploy AI skill: `career_advisor`
- [ ] ðŸ”² Deploy AI skill: `optimize_summary` (existing)
- [ ] ðŸ”² Endpoint: `GET /v1/templates`
- [ ] ðŸ”² Endpoint: `POST /v1/templates/recommend`
- [ ] ðŸ”² Endpoint: `POST /v1/copilot/chat`
- [ ] ðŸ”² Chat context gathering
- [ ] ðŸ”² Export engine (PDF/DOCX)

### Frontend Tasks
- [ ] ðŸ”² Template gallery
- [ ] ðŸ”² Template preview
- [ ] ðŸ”² AI recommendations
- [ ] ðŸ”² Template application
- [ ] ðŸ”² Career copilot chat UI
  - [ ] ðŸ”² Chat interface
  - [ ] ðŸ”² Context awareness
  - [ ] ðŸ”² Action items display
  - [ ] ðŸ”² Follow-up suggestions
- [ ] ðŸ”² Export modal with checklist
- [ ] ðŸ”² PDF/DOCX download

### Deliverables
- âœ… Users can select from 50+ templates
- âœ… Users get AI recommendations
- âœ… Users can chat with career copilot
- âœ… Users can export clean PDF/DOCX

### Success Criteria
- Template adoption > 70%
- Copilot engagement > 50%
- Export success rate > 98%

### Testing
- Test templates with 10+ recruiters
- Validate ATS compatibility of all templates
- Test exports across PDF readers

---

## ðŸ”„ Phase 7: Auto-Improvement Loop (Post-Launch)

### Goal
Enable safe, validated prompt optimization.

### Timeline
Start after collecting 1000+ AI requests (typically 4-6 weeks post-launch)

### Process
1. **Week 1: Data Collection**
   - [ ] ðŸ”² Sample 100 anonymized cases per skill
   - [ ] ðŸ”² Freeze inputs for reproducibility
   - [ ] ðŸ”² Run current prompts as baseline

2. **Week 2: Candidate Generation**
   - [ ] ðŸ”² Ask GPT-4 to suggest improvements
   - [ ] ðŸ”² Generate 3-5 candidate prompts per skill
   - [ ] ðŸ”² Store in `prompt_candidates` table

3. **Week 3: Evaluation**
   - [ ] ðŸ”² Run candidates on test set
   - [ ] ðŸ”² Collect evaluation metrics
   - [ ] ðŸ”² Calculate vs_current_delta
   - [ ] ðŸ”² Filter: delta > 5%, test_count > 100

4. **Week 4: Shadow Testing**
   - [ ] ðŸ”² Run winning candidates on live traffic
   - [ ] ðŸ”² No user impact (log only)
   - [ ] ðŸ”² Compare with production outputs

5. **Week 5: Promotion**
   - [ ] ðŸ”² Manual review by team
   - [ ] ðŸ”² Promote using `promote_prompt_to_production()`
   - [ ] ðŸ”² Monitor for 48 hours
   - [ ] ðŸ”² Rollback if issues

### Frequency
- First cycle: Manual (to validate process)
- Subsequent cycles: Bi-weekly

### Safety Checks
- No self-modification
- No real-time changes
- Always reversible
- Full audit trail

---

## ðŸ“Š Success Metrics by Phase

### Phase 1-2 (Foundation)
- **Auth Success**: > 98%
- **Upload Success**: > 95%
- **Parsing Accuracy**: > 90%
- **RLS Coverage**: 100%

### Phase 3 (AI Integration)
- **AI Response Time**: < 3s
- **Validation Rate**: > 95%
- **User Acceptance**: > 60%
- **Feedback Rate**: > 30%

### Phase 4 (Explainability)
- **Heatmap Accuracy**: > 85%
- **ATS Prediction Accuracy**: > 80%
- **Fix Adoption**: > 70%

### Phase 5 (Job Matching)
- **Gap Identification**: > 90% accurate
- **Match Score Correlation**: r > 0.7
- **User Helpfulness**: > 4/5 stars

### Phase 6 (Advanced)
- **Template Adoption**: > 70%
- **Copilot Engagement**: > 50%
- **Export Success**: > 98%

---

## ðŸš¨ Risk Mitigation

### Technical Risks
1. **AI Latency**
   - Mitigation: Cache common requests, use GPT-3.5 for simple tasks
   - Fallback: Deterministic suggestions

2. **Parsing Errors**
   - Mitigation: User review screen, confidence scores
   - Fallback: Manual editing always available

3. **Cost Overrun**
   - Mitigation: Per-user daily limits, cost alerts
   - Fallback: Pause AI features, use cheaper models

### Product Risks
1. **Low AI Adoption**
   - Mitigation: A/B test, improve UX, add tutorials
   - Metrics: Track per-feature adoption

2. **Trust Issues**
   - Mitigation: Explainability, show sources, allow feedback
   - Metrics: User satisfaction surveys

3. **Competitor Launch**
   - Mitigation: Focus on quality > speed, unique explainability
   - Differentiation: Safe auto-improvement

---

## ðŸ“… Launch Strategy

### Alpha (Week 8)
- 10 internal users
- Full feature set
- Daily feedback sessions
- Bug bash

### Beta (Week 10)
- 100 invited users
- Email waitlist
- Weekly surveys
- Feature usage tracking

### Public Launch (Week 12)
- Open signup
- Marketing push
- Press release
- Product Hunt launch

### Post-Launch (Week 13+)
- Monitor metrics
- Fix bugs
- Collect evaluations
- Start improvement loop

---

## ðŸŽ¯ Definition of Done

Each feature is "done" when:
1. âœ… Code reviewed and merged
2. âœ… Tests pass (unit + integration)
3. âœ… Documentation updated
4. âœ… Deployed to staging
5. âœ… QA tested
6. âœ… Product owner approved
7. âœ… Metrics dashboard created
8. âœ… Deployed to production

---

## ðŸ“ž Team Ceremonies

### Daily (15 min)
- Standup: What did, what will do, blockers
- Quick wins sharing

### Weekly (2 hours)
- Sprint planning
- Demo completed features
- Retrospective
- Metrics review

### Bi-Weekly (1 hour)
- User feedback review
- Roadmap adjustment
- Prompt performance review

---

## ðŸŽ‰ Milestones

- **Week 2**: âœ… Users can sign up and upload
- **Week 4**: âœ… Users can edit resumes
- **Week 6**: âœ… Users get AI suggestions
- **Week 8**: âœ… Alpha launch (10 users)
- **Week 10**: âœ… Beta launch (100 users)
- **Week 12**: ðŸš€ **PUBLIC LAUNCH**
- **Week 16**: âœ… First prompt improvement deployed

---

## ðŸ“š Resources

- [Integration Spec](FRONTEND_BACKEND_INTEGRATION_SPEC.md)
- [Supabase Setup](supabase/README.md)
- [Deployment Checklist](supabase/DEPLOYMENT_CHECKLIST.md)
- [Quick Reference](QUICK_REFERENCE.md)

---

**Ready to build? Start with Phase 1! ðŸš€**
